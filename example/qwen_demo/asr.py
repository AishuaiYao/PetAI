import pyaudio
import requests
import json
import base64
import time
import numpy as np

# --- é…ç½® ---
API_KEY = 'sk-943f95da67d04893b70c02be400e2935'
MODEL_NAME = "qwen3-asr-flash"
RESULT_FORMAT = "message"
API_URL = "https://dashscope.aliyuncs.com/api/v1/services/aigc/multimodal-generation/generation"

# éŸ³é¢‘å‚æ•°
CHUNK_SIZE = 3200
SAMPLE_RATE = 16000
CHANNELS = 1
AUDIO_FORMAT = pyaudio.paInt16
BYTES_PER_SAMPLE = 2

# VADå‚æ•°
SILENCE_THRESHOLD = 0.5
MIN_SPEECH_DURATION = 0.3
ENERGY_THRESHOLD_HIGH = 100000
ENERGY_THRESHOLD_LOW = 1000

# è¯·æ±‚å¤´
HEADERS = {
    'Authorization': f'Bearer {API_KEY}',
    'Content-Type': 'application/json'
}


def calculate_energy(audio_data):
    """è®¡ç®—éŸ³é¢‘èƒ½é‡"""
    samples = np.frombuffer(audio_data, dtype=np.int16)
    energy = np.sum(samples.astype(np.float32) ** 2) / len(samples)
    return energy


def print_energy_bar(energy, max_energy=5000, width=50):
    """æ‰“å°èƒ½é‡æ¡"""
    level = min(int((energy / max_energy) * width), width)
    bar = 'â–ˆ' * level + 'â–‘' * (width - level)
    status = "ğŸ”Š SPEAKING" if energy > ENERGY_THRESHOLD_HIGH else "ğŸ”ˆ LISTENING"
    print(f"\r[{bar}] {energy:6.0f} {status}", end='', flush=True)


def call_asr_api(audio_url):
    """ä½¿ç”¨requestsè°ƒç”¨ASR API - ç¬¬äºŒç§æ ¼å¼"""
    payload = {
        "model": MODEL_NAME,
        "input": {
            "messages": [
                {
                    "role": "user",
                    "content": [
                        {
                            "audio": audio_url
                        }
                    ]
                }
            ]
        },
        "parameters": {
            "result_format": RESULT_FORMAT
        }
    }


    response = requests.post(
        API_URL,
        headers=HEADERS,
        data=json.dumps(payload),
        timeout=30
    )

    if response.status_code == 200:
        result = response.json()
        # è§£æå“åº”
        try:
            # å°è¯•æ ‡å‡†ç»“æ„
            text = result['output']['choices'][0]['message']['content'][0]['text']
            return text, True
        except Exception as ex:
            print("error")
            return ex, False
    else:
        print(f"APIé”™è¯¯: {response.status_code}")
        print(f"é”™è¯¯ä¿¡æ¯: {response.text}")
        return None, False



def real_time_asr():
    """æ ¸å¿ƒæµå¼è¯†åˆ«é€»è¾‘"""
    p = pyaudio.PyAudio()

    stream = p.open(
        format=AUDIO_FORMAT,
        channels=CHANNELS,
        rate=SAMPLE_RATE,
        input=True,
        frames_per_buffer=CHUNK_SIZE
    )

    print("ğŸ¤ å¼€å§‹å½•éŸ³ï¼ŒVADæ¨¡å¼... (Ctrl+Cåœæ­¢)")
    print("=" * 50)
    print("èƒ½é‡æ˜¾ç¤ºï¼ˆå®æ—¶æ›´æ–°ï¼‰:")

    vad_state = "SILENT"
    speech_buffer = bytearray()
    silence_frames = 0
    speech_frames = 0
    call_count = 0
    last_text = ""

    frame_duration = CHUNK_SIZE / (SAMPLE_RATE * BYTES_PER_SAMPLE)

    try:
        while True:
            audio_data = stream.read(CHUNK_SIZE, exception_on_overflow=False)
            energy = calculate_energy(audio_data)

            # æ‰“å°èƒ½é‡æ¡
            print_energy_bar(energy)

            if vad_state == "SILENT":
                if energy > ENERGY_THRESHOLD_HIGH:
                    speech_frames += 1
                    if speech_frames * frame_duration >= MIN_SPEECH_DURATION:
                        vad_state = "SPEAKING"
                        print(f"\n\nğŸ”Š æ£€æµ‹åˆ°è¯­éŸ³å¼€å§‹ (èƒ½é‡: {energy:.0f})")
                        speech_buffer.extend(audio_data)
                else:
                    speech_frames = 0

            elif vad_state == "SPEAKING":
                speech_buffer.extend(audio_data)

                if energy < ENERGY_THRESHOLD_LOW:
                    silence_frames += 1
                    if silence_frames * frame_duration >= SILENCE_THRESHOLD:
                        vad_state = "SILENT"
                        silence_frames = 0
                        speech_frames = 0

                        if len(speech_buffer) > 0:
                            call_count += 1
                            audio_duration = len(speech_buffer) / (SAMPLE_RATE * BYTES_PER_SAMPLE)

                            print(f"\n\nğŸ“Š ç¬¬{call_count}æ¬¡è°ƒç”¨")
                            print(f"è¯­éŸ³æ®µ: {audio_duration:.2f}ç§’ ({len(speech_buffer)}å­—èŠ‚)")

                            # è½¬æ¢ä¸ºWAVæ ¼å¼
                            wav_data = speech_buffer
                            audio_b64 = base64.b64encode(wav_data).decode('utf-8')
                            audio_url = f"data:audio/wav;base64,{audio_b64}"

                            # è°ƒç”¨API
                            start_time = time.time()
                            text, success = call_asr_api(audio_url)
                            api_duration = time.time() - start_time

                            print(f"APIè€—æ—¶: {api_duration:.2f}ç§’")

                            if success and text:
                                print(f"âœ… è¯†åˆ«ç»“æœ: {text}")
                                last_text = text
                            else:
                                print(f"âŒ è¯†åˆ«å¤±è´¥: {text}")

                            print("-" * 50)
                            speech_buffer = bytearray()
                            print("\nç»§ç»­ç›‘å¬...")
                else:
                    silence_frames = 0

    except KeyboardInterrupt:
        print("\n\n" + "=" * 50)
        print("ğŸ¯ è¯†åˆ«ç»“æŸ")
        print(f"æ€»è°ƒç”¨æ¬¡æ•°: {call_count}")
        if last_text:
            print(f"æœ€åè¯†åˆ«ç»“æœ: {last_text}")
    except Exception as e:
        print(f"\nç¨‹åºå¼‚å¸¸: {e}")
        import traceback
        traceback.print_exc()
    finally:
        stream.stop_stream()
        stream.close()
        p.terminate()


if __name__ == "__main__":
    real_time_asr()